{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project\n",
    "\n",
    "#### Adrien DANEL & Slimane THABET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows our work related to this paper : [**Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?**](https://arxiv.org/abs/1711.02301)\n",
    "\n",
    "## Paper description\n",
    "\n",
    "The paper points out that developing learning algorithms that are robust across tasks and policy representations remains a\n",
    "challenge. While standard benchmarks like MuJoCo and Atari provide rich settings for experimentation, but the specifics of the underlying environments differ from each other in multiple ways, and thus determining the principles underlying any particular form of sub-optimal behavior is difficult. To evaluate the success of the alorithms we try on those environments, we usually uses **high scores** to determine those which are more performant, but it would be more interesting to look at **optimal behavior**, which is unfortunately not fully characterized and generally complex on this type of environment.\n",
    "\n",
    "The article proposes an new environment to better evaluate generalization, it is based on the work of Erdos and Selfridge, and Spencer. Erdos-Selfridge-Spencer (ESS) games are turn-based games where two players play adversarly. The cool thing is that **optimal behavior** (or strategy) can be defined by **potential functions** derived from conditional expectations over random future play.\n",
    "\n",
    "They chose to focus on a very well-known game type in the Reinforcement Learning, the **attacker-defender game**.\n",
    "\n",
    "The game is composed, at the beginning, of $N$ identical pieces ditributed on a board of $K$ levels. We are only interested of the level of each piece. Two assymetrical players compete: the ***attacker*** and the ***defender***. A turn of the game goes the following way :\n",
    "\n",
    "- The attacker separates the pieces in two sets $A$ and $B$ both with at least one piece.\n",
    "- The defender chooses one set to destroy and all pieces of the remaining set move up of one level.\n",
    "\n",
    "The attacker wins if **one (ore more) piece reaches the top**, and the defender wins if **all pieces are removed** or it remains only one piece on a level strictly below $K$.\n",
    "\n",
    "The following picture, taken from the paper, illustrates this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='schema_ess.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment specificities\n",
    "\n",
    "ESS games offer a nice field to experiment with RL techniques, because there exists several theoretical results which make evaluation of performance easier.\n",
    "\n",
    "Let us start with a definition.\n",
    "\n",
    "**Definition : potential function**\n",
    "\n",
    "Let us consider a state $\\mathcal{S} = (n_1, ..., n_k)$ where $n_i$ is the number of pieces in level $i$. We define then the ***potential*** of $\\mathcal{S}$, noted $\\phi(\\mathcal{S})$ by the following function:\n",
    "\n",
    "$$\\phi(\\mathcal{S}) = \\sum_{i=1}^{K}n_i2^{-(K-i)}$$\n",
    "\n",
    "**Reminder** : $K$ represents the number of levels.\n",
    "\n",
    "The potential is a key metric for results and strategies of ESS games.\n",
    "\n",
    "\n",
    "The possible outcomes and optimal strategies are in the following theorems:\n",
    "\n",
    "**Theorem 1**\n",
    "\n",
    "Let an instance of the ESS game with $N$ pieces and $K$ levels randomly placed on the board, and $\\mathcal{S}_0$ the initial state.\n",
    "\n",
    "- If $\\phi(\\mathcal{S}_0)<1$, the defender can always win\n",
    "- If $\\phi(\\mathcal{S}_0)\\geq1$, the attacker can always win\n",
    "\n",
    "If all pieces start at the bottom, this condition $\\phi(\\mathcal{S}_0)<1$ transforms to $N<2^K$.\n",
    "\n",
    "**Theorem 2**\n",
    "\n",
    "The optimal strategy for the defender is the following:\n",
    "\n",
    "- For the two proposed parts $A$ and $B$, compute $\\phi(A)$ and $\\phi(B)$.\n",
    "- Eliminate the part with the highest potential.\n",
    "\n",
    "\n",
    "For the rest of the work we will call *random* policy the policy where the defender chooses randomly between the sets $A$ and $B$, and the *optimal* policy the policy where it plays in an optimal way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our approach\n",
    "\n",
    "While the authors decided to create an environment based on the ESS using **[gym](https://gym.openai.com/)** (an **openAI** library for developing and comparing reinforcement learning algorithms on famous environments like **Cartpole** or **Pong**) and compare openAI algorithms on the new environment, we decided not to follow their path.\n",
    "\n",
    "We didn't think it was interesting to develop a gym-compatible ESS environment, and then use the already-made RL algorithms like DQN, PPO or A2C like what was done by the authors. It would have required some time to get use to the gym library and potentially come across errors unrelated to Reinforcement Learning.\n",
    "\n",
    "We chose then to develop our own RL algorithm on a non-gym ESS environment, that we would thus create from scratch and by doing so, allow us to focus on our algorithm.\n",
    "\n",
    "We chose to start by **training the defender**, and we suppose that the attacker always plays randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the Monte-Carlo method for policy evaluation and policy control without exploring start. The general algorithm is the following :\n",
    "\n",
    "<img src='MC_RL_algo.png' width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We slightly adapted this algorithm for the purpose of the game. For instance, it was difficult to enumerate every pair (state,action) from the begining due to the high number of possibilities. Furthermore, only 2 actions are available at every round for the defender, much less that all the theoretical pairs.\n",
    "\n",
    "The changes were then the following :\n",
    "\n",
    "- Not initializing Q for every $(s,a)$, but filled at the end of each game\n",
    "- For updating $\\pi$:\n",
    "    1. Look if the pairs $(s,a)$ and $(s,b)$ are in the table\n",
    "        * If both are in the table choose $argmax_{a,b} \\{Q(s,a), Q(s,b)\\}$\n",
    "        * If one is missing choose the one who is present\n",
    "        * If both are missing, choose one randomly\n",
    "    2. Play the chosen action with probability $1-\\epsilon$ and the other with probability $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from time import time\n",
    "from ESS import Game\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "   \n",
    "def hash_action(action):\n",
    "    '''Returns the action as a string to make a dictionnary key'''\n",
    "    return str(action)\n",
    "              \n",
    "def policy(Q, state, split_a, split_b, epsilon=0.1, play_random=False, play_optimal=False):\n",
    "    '''Play the policy'''\n",
    "    \n",
    "    #If we play random, choose a random part\n",
    "    if play_random:\n",
    "        if np.random.uniform()>0.5:\n",
    "            return split_a\n",
    "        else:\n",
    "            return split_b\n",
    "    #If we play optimal, return the part with highest potential  \n",
    "    if play_optimal:\n",
    "        weights = 0.5 ** (np.arange(len(split_a)))\n",
    "        pot_a = np.sum(weights*split_a)\n",
    "        pot_b = np.sum(weights*split_b)\n",
    "        if pot_a>=pot_b:\n",
    "            return split_a\n",
    "        else:\n",
    "            return split_b\n",
    "        \n",
    "    #Get the keys of the actions to look in the table\n",
    "    key_a = hash_action(split_a)\n",
    "    key_b = hash_action(split_b)\n",
    "    bigger = split_a\n",
    "    smaller = split_b\n",
    "    if np.random.uniform()>0.5:\n",
    "        bigger = split_b\n",
    "        smaller = split_a\n",
    "    \n",
    "    if state in Q.keys():\n",
    "        if key_a in Q[state].keys():\n",
    "            if key_b in Q[state].keys():\n",
    "                if Q[state][key_a] >= Q[state][key_b]:\n",
    "                    bigger = split_a\n",
    "                    smaller = split_b\n",
    "                else:\n",
    "                    bigger = split_b\n",
    "                    smaller = split_a\n",
    "            else:\n",
    "                bigger = split_a\n",
    "                smaller = split_b\n",
    "        else:\n",
    "            if key_b in Q[state].keys():\n",
    "                bigger = split_a\n",
    "                smaller = split_b\n",
    "                \n",
    "    #Whatever choice, flip it with epsilon probability          \n",
    "    if np.random.uniform()>epsilon:\n",
    "        return bigger\n",
    "    else:\n",
    "        return smaller\n",
    "    \n",
    "def evaluate_policy(N=1000, random=False, optimal=False, init_bottom=False):\n",
    "    winner_defender = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        game.reset(init_bottom=init_bottom)\n",
    "        winner=None\n",
    "        while winner != 'attacker' and winner != 'defender':\n",
    "            a,b = game.play_attacker()\n",
    "            action = policy(Q, game.getHash(), a, b, play_random=random, play_optimal=optimal)\n",
    "            assert (action==a).all() or (action==b).all()\n",
    "            game.play_defender(action)\n",
    "            winner = game.winner()\n",
    "            if winner=='defender':\n",
    "                winner_defender[i] = 1\n",
    "    return winner_defender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part plays the game, using the game environment we made in the [ESS.py](https://github.com/Adanel/ReinforcementLearningProject/blob/master/ESS.py) file. We initiate the game with a special condition which we highlighted before in the **Theorem 1** so that our defender **can always win**. We are in the case where all pieces start at the bottom. By doing so, we remove the hazard where our defender could loose whereas he plays optimally simply because of the pieces' initialisation. Thus the optimal policy represents 100% games won."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In random case, defender wins  89.9 % times\n",
      "After training, defender wins  99.3 % times\n",
      "In optimal case, defender wins  100.0 % times\n"
     ]
    }
   ],
   "source": [
    "#Initialize the game\n",
    "K=8 #Levels\n",
    "M=50 #Pieces\n",
    "game = Game(K, M, init_bottom=True)\n",
    "\n",
    "Q = {}\n",
    "returns = {}\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "#Number of Monte-Carlo iterations\n",
    "N = 10000\n",
    "\n",
    "defender_wins = []#Evaluation of the policy during the training\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    list_of_states = []\n",
    "    list_of_actions = []\n",
    "    list_of_rewards = []\n",
    "    list_of_returns = []\n",
    "    winner = None\n",
    "    game.reset(init_bottom=True)\n",
    "    \n",
    "    #play the game\n",
    "    while winner != 'attacker' and winner != 'defender':\n",
    "        a,b = game.play_attacker()\n",
    "        action = policy(Q, game.getHash(), a, b, epsilon=0.3)\n",
    "        assert (action==a).all() or (action==b).all()\n",
    "        list_of_states.append(game.getHash())\n",
    "        list_of_actions.append(hash_action(action))\n",
    "        game.play_defender(action)\n",
    "        winner = game.winner()\n",
    "        if winner =='attacker':\n",
    "            list_of_rewards.append(-1)\n",
    "        elif winner=='defender':\n",
    "            list_of_rewards.append(1)\n",
    "        else:\n",
    "            list_of_rewards.append(0)\n",
    "    \n",
    "    assert len(list_of_states) == len(list_of_rewards)\n",
    "    \n",
    "    #compute the returns\n",
    "    n = len(list_of_states)      \n",
    "    list_of_returns.append(0)\n",
    "    for j in range(1,n+1):\n",
    "        list_of_returns.append(list_of_rewards[n-j] + gamma * list_of_returns[j-1])\n",
    "    list_of_returns.reverse()\n",
    "    \n",
    "    assert len(list_of_states) == len(list_of_returns)-1\n",
    "    assert list_of_returns[-1]==0\n",
    "    #add returns to dictionnary\n",
    "    for j,s in enumerate(list_of_states):\n",
    "        a = list_of_actions[j]\n",
    "        if s in returns.keys():\n",
    "            if a in returns[s].keys():\n",
    "                returns[s][a].append(list_of_returns[j])\n",
    "            else:\n",
    "                returns[s][a] = [list_of_returns[j]]\n",
    "        else:\n",
    "            returns[s] = {}\n",
    "            returns[s][a] = [list_of_returns[j]]\n",
    "            \n",
    "    #Every 500 steps, update the policy       \n",
    "    if i%500==0:\n",
    "        for s in returns.keys():\n",
    "            Q[s] = dict()\n",
    "            for a in returns[s].keys():\n",
    "                Q[s][a] = np.mean(returns[s][a])\n",
    "        #keep results for plots, keep commented if not interested by the plots, it takes lots of times\n",
    "        #defender_wins.append(np.sum(evaluate_policy(init_bottom=True)))\n",
    "        \n",
    "        \n",
    "random_rate = np.sum(evaluate_policy(random=True, init_bottom=True))/10\n",
    "with_training_rate = np.sum(evaluate_policy(init_bottom=True))/10\n",
    "optimal_rate = np.sum(evaluate_policy(optimal=True, init_bottom=True))/10\n",
    "        \n",
    "\n",
    "#Check the result of training by comparing to random and optimal policy\n",
    "print(\"In random case, defender wins \", random_rate, \"% times\")  \n",
    "print(\"After training, defender wins \", with_training_rate, \"% times\")\n",
    "print(\"In optimal case, defender wins \", optimal_rate, \"% times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part can be used to display the progress of the winrate following the training. Because our algorithm takes time to learn depending on the Game's conditions, we also took screenshots of the plots and we show them underneath. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to plot results\n",
    "  \n",
    "#plt.figure()\n",
    "#plt.plot(np.arange(0,N,500), np.array(defender_wins)/10, label='learned policy')\n",
    "#plt.hlines(random_rate, 0, N, colors = 'g', label = \"random policy\")\n",
    "#plt.hlines(with_training_rate, 0, N, colors = 'b', label = 'learned policy')\n",
    "#plt.hlines(optimal_rate, 0, N, colors = 'r', label = 'optimal policy')\n",
    "#plt.xlabel(\"Steps of training\", fontsize='large')\n",
    "#plt.ylabel(\"% of victory in 1000 games\", fontsize='large')\n",
    "#plt.legend()\n",
    "#plt.axis([0,N,0,110])\n",
    "#plt.title(\"Performance for \"+str(K)+ \" levels and \"+str(M)+\" pieces\", fontsize='x-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are displayed the performance of our algorithm in different configurations. One can see that for few pieces, the random strategy is already very good, and the algorithm has very few to learn. For more pieces, the convergence is much slower, and we don't know if it converges at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='training_plot_5_8_10000_bottom.svg?sanitize=true' width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='training_plot_5_20_10000_bottom.svg?sanitize=true' width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='training_plot_8_500_60000_bottom.svg?sanitize=true' width=\"600\" height=\"400\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
