{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project\n",
    "\n",
    "#### Adrien DANEL & Slimane THABET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows our work related to this paper : [**Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?**](https://arxiv.org/abs/1711.02301)\n",
    "\n",
    "### Paper description\n",
    "\n",
    "The paper points out that developing learning algorithms that are robust across tasks and policy representations remains a\n",
    "challenge. While standard benchmarks like MuJoCo and Atari provide rich settings for experimentation, but the specifics of the underlying environments differ from each other in multiple ways, and thus determining the principles underlying any particular form of sub-optimal behavior is difficult. To evaluate the success of the alorithms we try on those environments, we usually uses **high scores** to determine those which are more performant, but it would be more interesting to look at **optimal behavior**, which is unfortunately not fully characterized and generally complex on this type of environment.\n",
    "\n",
    "The article proposes an new environment to better evaluate generalization, it is based on the work of Erdos and Selfridge, and Spencer. Erdos-Selfridge-Spencer (ESS) games are turn-based games where two players play adversarly. The cool thing is that **optimal behavior** (or strategy) can be defined by **potential functions** derived from conditional expectations over random future play.\n",
    "\n",
    "They chose to focus on a very well-known game type in the Reinforcement Learning, the **attacker-defender game**.\n",
    "\n",
    "The following picture, taken from the paper, explains how a turn works :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='schema_ess.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment specificities\n",
    "\n",
    "ESS games offer a nice field to experiment with RL techniques, because there exists several theoretical results which make evaluation of performance easier.\n",
    "\n",
    "Let us start with a definition.\n",
    "\n",
    "**Definition : potential function**\n",
    "\n",
    "Let us consider a state $\\mathcal{S} = (n_1, ..., n_k)$ where $n_i$ is the number of pieces in level $i$. We define then the *potential* of $\\mathcal{S}$, noted $\\phi(\\mathcal{S})$ by the following function:\n",
    "\n",
    "$$\\phi(\\mathcal{S}) = \\sum_{i=1}^{K}n_i2^{-(K-i)}$$\n",
    "\n",
    "The potential is a key metric for results and strategies of ESS games.\n",
    "\n",
    "\n",
    "The possible outcomes and optimal strategies are in the following theorems:\n",
    "\n",
    "**Theorem 1**\n",
    "\n",
    "Let an instance of the ESS game with $N$ pieces and $K$ levels randomly placed on the board, and $\\mathcal{S}_0$ the initial state.\n",
    "\n",
    "- If $\\phi(\\mathcal{S}_0)<1$, the defender can always win\n",
    "- If $\\phi(\\mathcal{S}_0)\\geq1$, the attacker can always win\n",
    "\n",
    "**Theorem 2**\n",
    "\n",
    "The optimal strategy for the defender is the following:\n",
    "\n",
    "- For the two proposed parts $A$ and $B$, compute $\\phi(A)$ and $\\phi(B)$.\n",
    "- Eliminate the part with the highest potential.\n",
    "\n",
    "\n",
    "For the rest of the work we will call *random* policy the policy where the defender chooses randomly between the sets $A$ and $B$, and the *optimal* policy the policy where it plays in an optimal way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What we chose to do\n",
    "\n",
    "While the authors decided to create an environment based on the ESS using **[gym](https://gym.openai.com/)** (an **openAI** library for developing and comparing reinforcement learning algorithms on famous environments like **Cartpole** or **Pong**) and compare openAI algorithms on the new environment, we decided to do somehting different.\n",
    "\n",
    "We didn't want to spend time on recreating a gym-compatible ESS environment, and then use the already-made RL algorithms like DQN, PPO or A2C. We felt like it was more interesting to develop our own RL algorithm on an ESS environment that we would also create from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented the Monte-Carlo method for policy evaluation and policy control without exploring start. The general algorithm is the following :\n",
    "\n",
    "<img src='MC_RL_algo.png' width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We slightly adapted this algorithm for the purpose of the game. For instance, it was difficult to enumerate every pair (state,action) from the begining due to the high number of possibilities. Furthermore, only 2 actions are available at every round for the defender, much less that all the theoretical pairs.\n",
    "\n",
    "The changes were then the following :\n",
    "\n",
    "- Not initializing Q for every $(s,a)$, but filled at the end of each game\n",
    "- For updating $\\pi$:\n",
    "    1. Look if the pairs $(s,a)$ and $(s,b)$ are in the table\n",
    "        * If both are in the table chose $argmax_{a,b} \\{Q(s,a), Q(s,b)\\}$\n",
    "        * If one is missing play chose the one who is present\n",
    "        * If both are missing, chose one randomly\n",
    "    2. Play the chosen action with probability $1-\\epsilon$ and the other with probability $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Feb 27 14:49:15 2020\n",
    "\n",
    "@author: slimane\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from time import time\n",
    "from ESS import Game\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "    \n",
    "def hash_action(action):\n",
    "    '''Returns the action as a string to make a dictionnary key'''\n",
    "    return str(action)\n",
    "       \n",
    " \n",
    "        \n",
    "def policy(Q, state, split_a, split_b, epsilon=0.1, play_random=False, play_optimal=False):\n",
    "    '''Play the policy'''\n",
    "    \n",
    "    #If we play random, choose a random part\n",
    "    if play_random:\n",
    "        if np.random.uniform()>0.5:\n",
    "            return split_a\n",
    "        else:\n",
    "            return split_b\n",
    "    #If we play optimal, return the part with highest potential  \n",
    "    if play_optimal:\n",
    "        weights = 0.5 ** (np.arange(len(split_a)))\n",
    "        pot_a = np.sum(weights*split_a)\n",
    "        pot_b = np.sum(weights*split_b)\n",
    "        if pot_a>=pot_b:\n",
    "            return split_a\n",
    "        else:\n",
    "            return split_b\n",
    "        \n",
    "    #Get the keys of the actions to look in the table\n",
    "    key_a = hash_action(split_a)\n",
    "    key_b = hash_action(split_b)\n",
    "    bigger = split_a\n",
    "    smaller = split_b\n",
    "    if np.random.uniform()>0.5:\n",
    "        bigger = split_b\n",
    "        smaller = split_a\n",
    "    \n",
    "    if state in Q.keys():\n",
    "        if key_a in Q[state].keys():\n",
    "            if key_b in Q[state].keys():\n",
    "                if Q[state][key_a] >= Q[state][key_b]:\n",
    "                    bigger = split_a\n",
    "                    smaller = split_b\n",
    "                else:\n",
    "                    bigger = split_b\n",
    "                    smaller = split_a\n",
    "            else:\n",
    "                bigger = split_a\n",
    "                smaller = split_b\n",
    "        else:\n",
    "            if key_b in Q[state].keys():\n",
    "                bigger = split_a\n",
    "                smaller = split_b\n",
    "                \n",
    "    #Whatever choice, flip it with epsilon probability          \n",
    "    if np.random.uniform()>epsilon:\n",
    "        return bigger\n",
    "    else:\n",
    "        return smaller\n",
    "    \n",
    "#Initialize the game\n",
    "game = Game(8, 500, init_bottom=True)\n",
    "\n",
    "Q = {}\n",
    "returns = {}\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "#Number of Monte-Carlo iterations\n",
    "N = 30000\n",
    "\n",
    "defender_wins = []#Evaluation of the policy during the training\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    list_of_states = []\n",
    "    list_of_actions = []\n",
    "    list_of_rewards = []\n",
    "    list_of_returns = []\n",
    "    winner = None\n",
    "    game.reset(init_bottom=True)\n",
    "    \n",
    "    #play the game\n",
    "    while winner != 'attacker' and winner != 'defender':\n",
    "        a,b = game.play_attacker()\n",
    "        action = policy(Q, game.getHash(), a, b, epsilon=0.3)\n",
    "        assert (action==a).all() or (action==b).all()\n",
    "        list_of_states.append(game.getHash())\n",
    "        list_of_actions.append(hash_action(action))\n",
    "        game.play_defender(action)\n",
    "        winner = game.winner()\n",
    "        if winner =='attacker':\n",
    "            list_of_rewards.append(-1)\n",
    "        elif winner=='defender':\n",
    "            list_of_rewards.append(1)\n",
    "        else:\n",
    "            list_of_rewards.append(0)\n",
    "    \n",
    "    assert len(list_of_states) == len(list_of_rewards)\n",
    "    \n",
    "    #compute the returns\n",
    "    n = len(list_of_states)      \n",
    "    list_of_returns.append(0)\n",
    "    for j in range(1,n+1):\n",
    "        list_of_returns.append(list_of_rewards[n-j] + gamma * list_of_returns[j-1])\n",
    "    list_of_returns.reverse()\n",
    "    \n",
    "    assert len(list_of_states) == len(list_of_returns)-1\n",
    "    assert list_of_returns[-1]==0\n",
    "    #add returns to dictionnary\n",
    "    for j,s in enumerate(list_of_states):\n",
    "        a = list_of_actions[j]\n",
    "        if s in returns.keys():\n",
    "            if a in returns[s].keys():\n",
    "                returns[s][a].append(list_of_returns[j])\n",
    "            else:\n",
    "                returns[s][a] = [list_of_returns[j]]\n",
    "        else:\n",
    "            returns[s] = {}\n",
    "            returns[s][a] = [list_of_returns[j]]\n",
    "            \n",
    "    #Every 500 steps, update the policy       \n",
    "    if i%500==0:\n",
    "        for s in returns.keys():\n",
    "            Q[s] = dict()\n",
    "            for a in returns[s].keys():\n",
    "                Q[s][a] = np.mean(returns[s][a])\n",
    "        defender_wins.append(np.sum(evaluate_policy(init_bottom=True)))\n",
    "        \n",
    "\n",
    "        \n",
    "def evaluate_policy(N=1000, random=False, optimal=False, init_bottom=False):\n",
    "    winner_defender = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        game.reset(init_bottom=init_bottom)\n",
    "        winner=None\n",
    "        while winner != 'attacker' and winner != 'defender':\n",
    "            a,b = game.play_attacker()\n",
    "            action = policy(Q, game.getHash(), a, b, play_random=random, play_optimal=optimal)\n",
    "            assert (action==a).all() or (action==b).all()\n",
    "            game.play_defender(action)\n",
    "            winner = game.winner()\n",
    "            if winner=='defender':\n",
    "                winner_defender[i] = 1\n",
    "    return winner_defender\n",
    "        \n",
    "\n",
    "#Check the result of training by comparing to random and optimal policy\n",
    "print(\"In random case, defender wins \", np.sum(evaluate_policy(random=True, init_bottom=True))/10, \"% times\")  \n",
    "print(\"After training, defender wins \", np.sum(evaluate_policy(init_bottom=True))/10, \"% times\")\n",
    "print(\"In optimal case, defender wins \", np.sum(evaluate_policy(optimal=True, init_bottom=True))/10, \"% times\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "- normal\n",
    "- cas ou le defendeur PEUT gagner ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='training_plot_8_500_bottom.svg?sanitize=true' width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
