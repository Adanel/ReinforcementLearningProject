{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project\n",
    "\n",
    "#### Adrien DANEL & Slimane THABET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows our work related to this paper : [**Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?**](https://arxiv.org/abs/1711.02301)\n",
    "\n",
    "#### Paper description\n",
    "\n",
    "The paper points out that developing learning algorithms that are robust across tasks and policy representations remains a\n",
    "challenge. While standard benchmarks like MuJoCo and Atari provide rich settings for experimentation, but the specifics of the underlying environments differ from each other in multiple ways, and thus determining the principles underlying any particular form of sub-optimal behavior is difficult. To evaluate the success of the alorithms we try on those environments, we usually uses **high scores** to determine those which are more performant, but it would be more interesting to look at **optimal behavior**, which is unfortunately not fully characterized and generally complex on this type of environment.\n",
    "\n",
    "The article proposes an new environment to better evaluate generalization, it is based on the work of Erdos and Selfridge, and Spencer. Erdos-Selfridge-Spencer (ESS) games are turn-based games where two players play adversarly. The cool thing is that **optimal behavior** (or strategy) can be defined by **potential functions** derived from conditional expectations over random future play.\n",
    "\n",
    "They chose to focus on a very well-known game type in the Reinforcement Learning, the **attacker-defender game**.\n",
    "\n",
    "The following picture, taken from the paper, explains how a turn works :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='schema_ess.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment specifities\n",
    "\n",
    "init, potential, theorems ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What we chose to do\n",
    "\n",
    "While the authors decided to create an environment based on the ESS using **[gym](https://gym.openai.com/)** (an **openAI** library for developing and comparing reinforcement learning algorithms on famous environments like **Cartpole** or **Pong**) and compare openAI algorithms on the new environment, we decided to do somehting different.\n",
    "\n",
    "We didn't want to spend time on recreating a gym-compatible ESS environment, and then use the already-made RL algorithms like DQN, PPO or A2C. We felt like it was more interesting to develop our own RL algorithm on an ESS environment that we would also create from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "- normal\n",
    "- cas ou le defendeur PEUT gagner ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
